{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import time\n",
    "import numpy as np \n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.widgets import Button\n",
    "import matplotlib.dates as mdates\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from gym import spaces\n",
    "\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self,table_num =  0,  window_size=200):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        \n",
    "        \n",
    "        # today = \"A\" + datetime.now().strftime(\"%Y%m%d\") \n",
    "        self.host = '127.0.0.1'\n",
    "        self.user = 'root'\n",
    "        self.databaseA = 'a20240112'\n",
    "        self.password = '93150lbm!!'\n",
    "        # sample_code = '452190'\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.current_step = window_size\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2) \n",
    "\n",
    "        self.tables = [code for (code,) in self.get_table_names(self.host,self.user,self.password,self.databaseA)]\n",
    "        \n",
    "        self.sample_code = self.tables[table_num]\n",
    "        \n",
    "        \n",
    "        self.df = self.table_to_dataframe(self.host,self.user,self.password,self.databaseA,self.sample_code)\n",
    "        self.process_df = self.preprocessing(self.df)\n",
    "        print(self.process_df)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.reward = 0 \n",
    "        self.done = False\n",
    "        current_price = 'Fluctuation_15s'\n",
    "        \n",
    "        \n",
    "        fluctuation = self.process_df['Fluctuation_15s'][self.current_step]\n",
    "        \n",
    "        if action == 0 : # buy \n",
    "            if fluctuation > 0 : \n",
    "                self.reward = fluctuation\n",
    "            elif fluctuation <= 0 : \n",
    "                self.reward = - fluctuation\n",
    "        elif action == 1 : # sell\n",
    "            if fluctuation < 0 : \n",
    "                self.reward = - fluctuation\n",
    "            elif fluctuation > 0 : \n",
    "                self.reward = fluctuation\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.current_step >= len(self.process_df)-1:\n",
    "            self.done = True \n",
    "        \n",
    "        else : \n",
    "            self.current_step += 1\n",
    "        \n",
    "        return self._next_observation(), self.reward, self.done, {}\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size\n",
    "        self.current_step += 1\n",
    "        return self._next_observation()\n",
    "        \n",
    "    def _next_observation(self):\n",
    "        # end = min(len(self.process_df), self.current_step)  # Ensure end does not exceed DataFrame length\n",
    "        # start = max(0, end - self.window_size)\n",
    "        start = self.current_step - self.window_size\n",
    "        end = self.current_step\n",
    "        obs = self.process_df.loc[start:end-1,'signal'].values\n",
    "        obs = np.array(obs)\n",
    "        # print(f\"Start: {start}, End: {end}, Obs length: {len(obs)}\")\n",
    "        return obs\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def preprocessing(self,df):\n",
    "        df['체결시간'] = pd.to_datetime(df['체결시간'])\n",
    "        df['현재가'] = pd.to_numeric(df['현재가'].str.replace('+','').str.replace('-',''), errors='coerce')\n",
    "        df['거래량'] = pd.to_numeric(df['거래량'], errors='coerce')\n",
    "        df['거래대금'] = df['현재가'] * df['거래량']\n",
    "        df = df.sort_values(by='체결시간', ignore_index=True)\n",
    "        df['time_diff_seconds'] = df['체결시간'].diff().dt.total_seconds()\n",
    "        df['exp_minus'] = np.exp(-df['time_diff_seconds'])\n",
    "        df['signal'] = df['exp_minus'] * df['거래대금']\n",
    "\n",
    "        \n",
    "        df_use = df[['체결시간','거래대금', 'time_diff_seconds', '현재가','signal']]\n",
    "        \n",
    "        \n",
    "        df_use['Fluctuation_15s'] = None\n",
    "\n",
    "        \n",
    "        intervals = 15  # 15 seconds, 30 seconds, 1 minute\n",
    "        seconds = intervals \n",
    "        column_name = 'Fluctuation_15s'\n",
    "        for index, row in tqdm(df_use.iterrows(),total= len(df_use)):\n",
    "            base_time = row['체결시간']\n",
    "            comparison_index = self.find_closest_index(df_use, base_time, seconds)\n",
    "            fluctuation_rate = self.calculate_fluctuation_rate(df_use, index, comparison_index)\n",
    "            df_use.at[index, column_name] = fluctuation_rate\n",
    "        \n",
    "        df_use = df_use.dropna()\n",
    "        \n",
    "        filtered_df = df_use[df_use['체결시간'].dt.time < pd.to_datetime('15:19').time()]\n",
    "\n",
    "        filtered_df = filtered_df.dropna()\n",
    "        # df_use['Fluctuation_15s'] = None\n",
    "        # df_use['Fluctuation_30s'] = None\n",
    "        # df_use['Fluctuation_60s'] = None\n",
    "        \n",
    "        # intervals = [15, 30, 60]  # 15 seconds, 30 seconds, 1 minute\n",
    "        # for index, row in tqdm(df_use.iterrows(),total= len(df_use)):\n",
    "        #     base_time = row['체결시간']\n",
    "        #     for seconds, column_name in zip(intervals, ['Fluctuation_15s', 'Fluctuation_30s', 'Fluctuation_60s']):\n",
    "        #         comparison_index = self.find_closest_index(df_use, base_time, seconds)\n",
    "        #         fluctuation_rate = self.calculate_fluctuation_rate(df_use, index, comparison_index)\n",
    "        #         df_use.at[index, column_name] = fluctuation_rate\n",
    "        \n",
    "        # df_use = df_use.dropna()\n",
    "        \n",
    "        \n",
    "        return filtered_df \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def find_closest_index(self,df, base_time, delta_seconds):\n",
    "        target_time = base_time + pd.Timedelta(seconds=delta_seconds)\n",
    "        closest_index = df['체결시간'].searchsorted(target_time, side='left')\n",
    "        if closest_index < len(df):\n",
    "            return closest_index\n",
    "        return None  # Indicates no suitable index was found\n",
    "\n",
    "    # Calculate fluctuation rate\n",
    "    def calculate_fluctuation_rate(self,df, base_index, comparison_index):\n",
    "        if comparison_index is None or comparison_index >= len(df):\n",
    "            return np.nan  # No data available for comparison\n",
    "        initial_price = df.iloc[base_index]['현재가']\n",
    "        new_price = df.iloc[comparison_index]['현재가']\n",
    "        return ((new_price - initial_price) / initial_price) * 100\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def table_to_dataframe(self,host, user, password, database, table_name):\n",
    "        conn = None\n",
    "        try:\n",
    "            # Establish a database connection\n",
    "            conn = mysql.connector.connect(\n",
    "                host=host,\n",
    "                user=user,\n",
    "                password=password,\n",
    "                database=database\n",
    "            )\n",
    "\n",
    "            # Use backticks around the table name in the query\n",
    "            query = f\"SELECT * FROM `{table_name}`\"\n",
    "\n",
    "            # Use pandas to read the query into a DataFrame\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                df = pd.read_sql(query, conn)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except mysql.connector.Error as error:\n",
    "            print(f\"Error: {error}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if conn.is_connected():\n",
    "                conn.close()\n",
    "        \n",
    "\n",
    "    def get_table_names(self, host, user, password, database):\n",
    "        conn = None\n",
    "        cursor = None\n",
    "        tables = []  # Initialize tables as an empty list\n",
    "\n",
    "        try:\n",
    "            # Establish a database connection\n",
    "            conn = mysql.connector.connect(\n",
    "                host=host,\n",
    "                user=user,\n",
    "                password=password,\n",
    "                database=database\n",
    "            )\n",
    "\n",
    "            # Create a cursor object\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Execute a query to retrieve table names\n",
    "            query = f\"SELECT table_name FROM INFORMATION_SCHEMA.TABLES WHERE table_schema = '{database}'\"\n",
    "            cursor.execute(query)\n",
    "\n",
    "            # Fetch all the rows\n",
    "            tables = cursor.fetchall()\n",
    "\n",
    "        except mysql.connector.Error as error:\n",
    "            print(f\"Error: {error}\")\n",
    "\n",
    "        finally:\n",
    "            # Ensure to close resources properly\n",
    "            if cursor is not None:\n",
    "                cursor.close()\n",
    "            if conn is not None and conn.is_connected():\n",
    "                conn.close()\n",
    "\n",
    "        return tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pari0\\AppData\\Local\\Temp\\ipykernel_25684\\1268794650.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_use['Fluctuation_15s'] = None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c493265e20c42789ed282c2f534c3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2738 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           체결시간     거래대금  time_diff_seconds   현재가  \\\n",
      "1    2024-01-12 09:41:55.207204 -1856160           4.808627  1440   \n",
      "2    2024-01-12 09:41:55.207204 -8398004           0.000000  1439   \n",
      "3    2024-01-12 09:41:55.208206   -41702           0.001002  1438   \n",
      "4    2024-01-12 09:41:55.208206 -2838075           0.000000  1437   \n",
      "5    2024-01-12 09:41:57.293832 -1000632           2.085626  1446   \n",
      "...                         ...      ...                ...   ...   \n",
      "2693 2024-01-12 15:18:53.396575   447900           4.499842  1493   \n",
      "2694 2024-01-12 15:18:53.791248   -74600           0.394673  1492   \n",
      "2695 2024-01-12 15:18:55.329681     2986           1.538433  1493   \n",
      "2696 2024-01-12 15:18:58.803315     1493           3.473634  1493   \n",
      "2697 2024-01-12 15:18:58.928480    14930           0.125165  1493   \n",
      "\n",
      "            signal Fluctuation_15s  \n",
      "1    -1.514451e+04       -0.138889  \n",
      "2    -8.398004e+06       -0.069493  \n",
      "3    -4.166024e+04             0.0  \n",
      "4    -2.838075e+06        0.069589  \n",
      "5    -1.243078e+05        -0.55325  \n",
      "...            ...             ...  \n",
      "2693  4.976506e+03             0.0  \n",
      "2694 -5.027297e+04        0.067024  \n",
      "2695  6.411459e+02             0.0  \n",
      "2696  4.628921e+01             0.0  \n",
      "2697  1.317350e+04             0.0  \n",
      "\n",
      "[2697 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "env = TradingEnv(table_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# matplotlib 설정\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# GPU를 사용할 경우\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128*2)\n",
    "        self.layer2 = nn.Linear(128*2, 128*4)\n",
    "        self.layer3 = nn.Linear(128*4, 128*2)\n",
    "        self.layer4 = nn.Linear(128*2, 128)\n",
    "        self.layer5 = nn.Linear(128, 64)\n",
    "        self.layer6 = nn.Linear(64, n_actions)\n",
    "\n",
    "    # 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됩니다.\n",
    "    # ([[left0exp,right0exp]...]) 를 반환합니다.\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x)) \n",
    "        x = F.relu(self.layer2(x)) \n",
    "        x = F.relu(self.layer3(x)) \n",
    "        x = F.relu(self.layer4(x)) \n",
    "        x = F.relu(self.layer5(x)) \n",
    "        return self.layer6(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE는 리플레이 버퍼에서 샘플링된 트랜지션의 수입니다.\n",
    "# GAMMA는 이전 섹션에서 언급한 할인 계수입니다.\n",
    "# EPS_START는 엡실론의 시작 값입니다.\n",
    "# EPS_END는 엡실론의 최종 값입니다.\n",
    "# EPS_DECAY는 엡실론의 지수 감쇠(exponential decay) 속도 제어하며, 높을수록 감쇠 속도가 느립니다.\n",
    "# TAU는 목표 네트워크의 업데이트 속도입니다.\n",
    "# LR은 ``AdamW`` 옵티마이저의 학습율(learning rate)입니다.\n",
    "BATCH_SIZE = 1024\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
    "n_actions = env.action_space.n\n",
    "# 상태 관측 횟수를 얻습니다.\n",
    "state = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "\n",
    "# print(state)\n",
    "# print(type(state))\n",
    "# print(state.shape)\n",
    "\n",
    "\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "\n",
    "policy_net.load_state_dict(torch.load(f'policy_net.pth'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(260000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
    "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
    "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
    "            \n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
    "    # 전환합니다.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\n",
    "    # (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
    "    # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
    "    # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
    "    # max(1)[0]으로 최고의 보상을 선택하십시오.\n",
    "    # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # 기대 Q 값 계산\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber 손실 계산\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # 모델 최적화\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # 변화도 클리핑 바꿔치기\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step :  1000, mid_acc : 74.00%\n",
      "Step :  2000, mid_acc : 69.30%\n",
      "acc : 67.99%\n"
     ]
    }
   ],
   "source": [
    "true_count = 0 \n",
    "total_cnt = 0 \n",
    "done = False\n",
    "N = 1000\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # 환경과 상태 초기화\n",
    "    state = env.reset()\n",
    "    \n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    while not  done  : \n",
    "    \n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # 메모리에 변이 저장\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        if reward > 0 : \n",
    "            true_count += 1\n",
    "        \n",
    "        total_cnt += 1      \n",
    "        \n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "        # (정책 네트워크에서) 최적화 한단계 수행\n",
    "        optimize_model()\n",
    "\n",
    "        # 목표 네트워크의 가중치를 소프트 업데이트\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        \n",
    "        if total_cnt % N == 0:  # Replace N with the frequency of prints (e.g., 100)\n",
    "            print(f\"Step :  {total_cnt}, mid_acc : {true_count/total_cnt*100:.2f}%\")\n",
    "        \n",
    "        if done : \n",
    "            print(f'acc : {true_count/total_cnt*100:.2f}%')\n",
    "            \n",
    "     \n",
    "    torch.save(policy_net.state_dict(), f'policy_net.pth')\n",
    "    break\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layer1): Linear(in_features=200, out_features=256, bias=True)\n",
       "  (layer2): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (layer3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (layer4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (layer5): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer6): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN(n_observations,n_actions)\n",
    "model.load_state_dict(torch.load(f'policy_net.pth'))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f365f32be649fc9b7aab99a48f1987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2697 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.90%\n"
     ]
    }
   ],
   "source": [
    "total_cnt= 0 \n",
    "true_cnt = 0 \n",
    "\n",
    "for i in tqdm(range(len(env.process_df))):\n",
    "\n",
    "    try: \n",
    "        signal_data = env.process_df['signal'][i:i+200].values\n",
    "        fluc = env.process_df['Fluctuation_15s'][i:i+200].values[-1]\n",
    "\n",
    "\n",
    "        input_data = torch.tensor(signal_data,dtype=torch.float32)\n",
    "        action = model(input_data)\n",
    "        # print(fluc)\n",
    "        # print(action.argmax())\n",
    "        if fluc > 0 and action.argmax() > 0 :\n",
    "            true_cnt += 1\n",
    "            \n",
    "        elif fluc < 0 and action.argmax() < 0 : \n",
    "            total_cnt +=1\n",
    "        \n",
    "    except : \n",
    "        break\n",
    "    \n",
    "print(f'{true_cnt/total_cnt*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pari0\\AppData\\Local\\Temp\\ipykernel_25684\\1268794650.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_use['Fluctuation_15s'] = None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3339ff4c363412899288940efcb0566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           체결시간     거래대금  time_diff_seconds   현재가  \\\n",
      "1    2024-01-12 09:41:33.675269    47000           0.523127  4700   \n",
      "2    2024-01-12 09:41:34.103196    94000           0.427927  4700   \n",
      "3    2024-01-12 09:41:34.655392  1099800           0.552196  4700   \n",
      "4    2024-01-12 09:41:35.133178    18800           0.477786  4700   \n",
      "5    2024-01-12 09:41:35.278073    47000           0.144895  4700   \n",
      "...                         ...      ...                ...   ...   \n",
      "7188 2024-01-12 15:18:52.093593  9190000          11.997245  4595   \n",
      "7189 2024-01-12 15:18:54.330773  5004800           2.237180  4600   \n",
      "7190 2024-01-12 15:18:58.101781  4995600           3.771008  4600   \n",
      "7191 2024-01-12 15:18:59.392030    -4590           1.290249  4590   \n",
      "7192 2024-01-12 15:18:59.602439   -64260           0.210409  4590   \n",
      "\n",
      "             signal Fluctuation_15s  \n",
      "1      27855.226134        0.106383  \n",
      "2      61274.745884        0.106383  \n",
      "3     633137.503754        0.106383  \n",
      "4      11658.912043        0.106383  \n",
      "5      40660.316885        0.106383  \n",
      "...             ...             ...  \n",
      "7188      56.621088        0.108814  \n",
      "7189  534308.149178       -0.217391  \n",
      "7190  115042.865684       -0.217391  \n",
      "7191   -1263.178324             0.0  \n",
      "7192  -52066.843951             0.0  \n",
      "\n",
      "[7192 rows x 6 columns]\n",
      "Step :  1000, mid_acc : 64.80%\n",
      "Step :  2000, mid_acc : 59.70%\n",
      "Step :  3000, mid_acc : 56.77%\n",
      "Step :  4000, mid_acc : 52.18%\n",
      "Step :  5000, mid_acc : 49.48%\n",
      "Step :  6000, mid_acc : 49.58%\n",
      "acc : 49.12%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428fd9da6c0b405e82c1dd742cd75a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL : 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pari0\\AppData\\Local\\Temp\\ipykernel_25684\\1268794650.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_use['Fluctuation_15s'] = None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b99f6d2dbe746839cccb16cfd4f1e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            체결시간      거래대금  time_diff_seconds    현재가  \\\n",
      "1     2024-01-12 09:41:34.070282   -241600           0.992402  60400   \n",
      "2     2024-01-12 09:41:35.085073  -5375600           1.014791  60400   \n",
      "3     2024-01-12 09:41:35.219073    302500           0.134000  60500   \n",
      "4     2024-01-12 09:41:35.436073  -1328800           0.217000  60400   \n",
      "5     2024-01-12 09:41:35.701061 -13771200           0.264988  60400   \n",
      "...                          ...       ...                ...    ...   \n",
      "27203 2024-01-12 15:18:56.091254  29836800           0.090630  59200   \n",
      "27204 2024-01-12 15:18:56.092252   2960000           0.000998  59200   \n",
      "27205 2024-01-12 15:18:56.092252    947200           0.000000  59200   \n",
      "27206 2024-01-12 15:18:56.092252    296000           0.000000  59200   \n",
      "27207 2024-01-12 15:18:56.092252  10123200           0.000000  59200   \n",
      "\n",
      "             signal Fluctuation_15s  \n",
      "1     -8.955755e+04        0.165563  \n",
      "2     -1.948538e+06        0.165563  \n",
      "3      2.645635e+05             0.0  \n",
      "4     -1.069591e+06        0.165563  \n",
      "5     -1.056547e+07        0.165563  \n",
      "...             ...             ...  \n",
      "27203  2.725161e+07             0.0  \n",
      "27204  2.957047e+06             0.0  \n",
      "27205  9.472000e+05             0.0  \n",
      "27206  2.960000e+05             0.0  \n",
      "27207  1.012320e+07             0.0  \n",
      "\n",
      "[27207 rows x 6 columns]\n",
      "Step :  1000, mid_acc : 70.60%\n",
      "Step :  2000, mid_acc : 63.55%\n",
      "Step :  3000, mid_acc : 68.07%\n",
      "Step :  4000, mid_acc : 66.25%\n",
      "Step :  5000, mid_acc : 66.22%\n",
      "Step :  6000, mid_acc : 65.23%\n",
      "Step :  7000, mid_acc : 63.44%\n",
      "Step :  8000, mid_acc : 63.25%\n",
      "Step :  9000, mid_acc : 61.97%\n",
      "Step :  10000, mid_acc : 60.63%\n",
      "Step :  11000, mid_acc : 59.51%\n",
      "Step :  12000, mid_acc : 61.92%\n",
      "Step :  13000, mid_acc : 63.73%\n",
      "Step :  14000, mid_acc : 63.97%\n",
      "Step :  15000, mid_acc : 64.49%\n",
      "Step :  16000, mid_acc : 65.61%\n",
      "Step :  17000, mid_acc : 66.89%\n",
      "Step :  18000, mid_acc : 67.30%\n",
      "Step :  19000, mid_acc : 67.85%\n",
      "Step :  20000, mid_acc : 67.77%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# (정책 네트워크에서) 최적화 한단계 수행\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 목표 네트워크의 가중치를 소프트 업데이트\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[0;32m     74\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m non_final_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m s: s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     13\u001b[0m                                       batch\u001b[38;5;241m.\u001b[39mnext_state)), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[0;32m     14\u001b[0m non_final_next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mnext_state\n\u001b[0;32m     15\u001b[0m                                             \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m---> 16\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39maction)\n\u001b[0;32m     18\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39mreward)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(30,40):\n",
    "\n",
    "\n",
    "\n",
    "    env = TradingEnv(table_num=i)\n",
    "    \n",
    "    BATCH_SIZE = 1024\n",
    "    GAMMA = 0.99\n",
    "    EPS_START = 0.9\n",
    "    EPS_END = 0.05\n",
    "    EPS_DECAY = 1000\n",
    "    TAU = 0.005\n",
    "    LR = 1e-4\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    state = env.reset()\n",
    "    n_observations = len(state)\n",
    "\n",
    "\n",
    "\n",
    "    policy_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net = DQN(n_observations, n_actions).to(device)\n",
    "\n",
    "    policy_net.load_state_dict(torch.load(f'policy_net.pth'))\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "    memory = ReplayMemory(260000)\n",
    "\n",
    "\n",
    "    steps_done = 0\n",
    "    \n",
    "    true_count = 0 \n",
    "    total_cnt = 0 \n",
    "    done = False\n",
    "    N = 1000\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "    while not  done  : \n",
    "\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # 메모리에 변이 저장\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        if reward > 0 : \n",
    "            true_count += 1\n",
    "        \n",
    "        total_cnt += 1      \n",
    "        \n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "        # (정책 네트워크에서) 최적화 한단계 수행\n",
    "        optimize_model()\n",
    "\n",
    "        # 목표 네트워크의 가중치를 소프트 업데이트\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        \n",
    "        if total_cnt % N == 0:  # Replace N with the frequency of prints (e.g., 100)\n",
    "            print(f\"Step :  {total_cnt}, mid_acc : {true_count/total_cnt*100:.2f}%\")\n",
    "        \n",
    "        if done : \n",
    "            print(f'acc : {true_count/total_cnt*100:.2f}%')\n",
    "    \n",
    "    \n",
    "    total_cnt= 0 \n",
    "    true_cnt = 0 \n",
    "\n",
    "    for i in tqdm(range(len(env.process_df))):\n",
    "        total_cnt += 1 \n",
    "        try: \n",
    "            signal_data = env.process_df['signal'][i:i+200].values\n",
    "            fluc = env.process_df['Fluctuation_15s'][i:i+200].values[-1]\n",
    "\n",
    "\n",
    "            input_data = torch.tensor(signal_data,dtype=torch.float32)\n",
    "            action = model(input_data)\n",
    "            # print(fluc)\n",
    "            # print(action.argmax())\n",
    "            if fluc > 0 and action.argmax() == 0 :\n",
    "                true_cnt += 1\n",
    "                \n",
    "            elif fluc < 0 and action.argmax() == 1 : \n",
    "                true_cnt +=1\n",
    "            \n",
    "        except : \n",
    "            break\n",
    "        \n",
    "        \n",
    "    print(f'EVAL : {true_cnt/total_cnt*100:.2f}%')     \n",
    "    torch.save(policy_net.state_dict(), f'policy_net.pth')\n",
    "\n",
    "        \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
